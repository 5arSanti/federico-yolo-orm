{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/5arSanti/federico-object-recognition-model/blob/main/Train_YOLO_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, yaml\n",
        "import glob\n",
        "import os, shutil, pathlib\n",
        "from zipfile import ZipFile\n",
        "from utils.create_data_yaml import create_data_yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sUfcA8ZgR2t"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This notebook uses [Ultralytics](https://docs.ultralytics.com/) to train YOLO11, YOLOv8, or YOLOv5 object detection models with a custom dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NW7LLv_QPOO"
      },
      "source": [
        "**Verify NVIDIA GPU Availability**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfaWho47RGDf",
        "outputId": "3939810c-b265-4c66-d2d3-2e512309453f"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPZEM27IOh79"
      },
      "source": [
        "**Option 1. Upload the Dataset**\n",
        "\n",
        "Upload the `data.zip` (Export from Label studio) to the root folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7Iz9eBzW5zm"
      },
      "source": [
        "## 2. Split images into train and validation folders\n",
        "Next, we'll unzip `data.zip` and create some folders to hold the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8O6z-wVcPEF"
      },
      "outputs": [],
      "source": [
        "# Unzip images to a custom data folder\n",
        "!unzip -q dataset/data.zip -d dataset/custom_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoPjqW6AYebn"
      },
      "source": [
        "Afterwards and taking into account that we use Ultralytics, it is needed particular folder structure to store training data for models. The root folder is named “data”. Inside, there are two main folders:\n",
        "\n",
        "*   **Train**: These are the actual images used to train the model. In one epoch of training, every image in the train set is passed into the neural network. The training algorithm adjusts the network weights to fit the data in the images.\n",
        "\n",
        "*   **Validation**: These images are used to check the model's performance at the end of each training epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8X62eFTugosf"
      },
      "outputs": [],
      "source": [
        "!python utils/train_val_split.py --datapath=\"dataset/custom_data\" --train_pct=0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuZoMkSFN9XG"
      },
      "source": [
        "# 4.&nbsp;Configure Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c5Kdh0GmQHS"
      },
      "source": [
        "There's one last step before we can run training: we need to create the Ultralytics training configuration YAML file. This file specifies the location of your train and validation data, and it also defines the model's classes.\n",
        "\n",
        "It is necessary to ensure that the Labelmap is located in `custom_data/classes.txt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4letvP7X12ji"
      },
      "outputs": [],
      "source": [
        "path_to_classes_txt = 'dataset/custom_data/classes.txt'\n",
        "path_to_data_yaml = 'data.yaml'\n",
        "\n",
        "create_data_yaml(path_to_classes_txt, path_to_data_yaml)\n",
        "\n",
        "print('\\nFile contents:\\n')\n",
        "!cat data.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myP80_bnTNMi"
      },
      "source": [
        "# 5.&nbsp;Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfKspYasCzC8"
      },
      "source": [
        "## 5.1 Training Parameters\n",
        "\n",
        "**Model architecture & size (`model`):**\n",
        "\n",
        "There are several YOLO11 models sizes available to train, including `yolo11n.pt`, `yolo11s.pt`, `yolo11m.pt`, `yolo11l.pt`, and `yolo11xl.pt`. Larger models run slower but have higher accuracy, while smaller models run faster but have lower accuracy. \n",
        "\n",
        "Example: [check it out here to get a sense of their speed accuracy](https://youtu.be/_WKS4E9SmkA). \n",
        "By default, `yolo11n.pt` will be used as the starting point.\n",
        "\n",
        "We can also train with YOLOv8 or YOLOv5 models by substituting `yolo11` for `yolov8` or `yolov5`.\n",
        "\n",
        "\n",
        "**Number of epochs (`epochs`)**\n",
        "In machine learning, one “epoch” is one single pass through the full training dataset. Setting the number of epochs dictates how long the model will train for. The best amount of epochs to use depends on the size of the dataset and the model architecture. \n",
        "\n",
        "In our case we will use 60 epochs. \n",
        "If your dataset has more than 200 images, a good starting point is 40 epochs.\n",
        "\n",
        "\n",
        "**Resolution (`imgsz`)**\n",
        "Resolution has a large impact on the speed and accuracy of the model: a lower resolution model will have higher speed but less accuracy. \n",
        "YOLO models are typically trained and inferenced at a 640x640 resolution. \n",
        "\n",
        "We can choose to lower the resolution to 480x480 for better model performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQi_hXnUVPr-"
      },
      "source": [
        "model = Specifies the model to use\n",
        "epochs = Specifies the number of training epochs\n",
        "imgsz = Specifies the resolution of the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bbpob1gTPlo"
      },
      "outputs": [],
      "source": [
        "!yolo detect train data=data.yaml model=yolo11n.pt epochs=60 imgsz=640"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv0EYWJ5V6mC"
      },
      "source": [
        "The training algorithm will parse the images in the training and validation directories and then start training the model. \n",
        "\n",
        "At the end of each training epoch, the program runs the model on the validation dataset and reports the resulting mAP, precision, and recall. As training continues, the mAP should generally increase with each epoch. Training will end once it goes through the number of epochs specified.\n",
        "\n",
        "The best trained model weights will be saved in `content/runs/detect/train/weights/best.pt`. Additional information about training is saved in the `content/runs/detect/train` folder, including a `results.png` file that shows how loss, precision, recall, and mAP progressed over each epoch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo8BJRXeg0Ap"
      },
      "source": [
        "# 6.&nbsp;Test Model\n",
        "The commands below run the model on the images in the validation folder and then display the results for the first 5 images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PooP5Vjsg2Jn"
      },
      "outputs": [],
      "source": [
        "!yolo detect predict model=runs/detect/train/weights/best.pt source=data/validation/images save=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEEObQqoiGrs"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "for image_path in glob.glob(f'runs/detect/predict/*.jpg')[:5]:\n",
        "  display(Image(filename=image_path, height=400))\n",
        "  print('\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGiQw_gWbSBa"
      },
      "source": [
        "The model should draw a box around each object of interest in each image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7yrFRViVczX"
      },
      "source": [
        "# 7.&nbsp;Deploy Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcoBAeHXa86W"
      },
      "source": [
        "## 7.1 Download YOLO Model\n",
        "\n",
        "First, zip and download the trained model by running the code blocks below.\n",
        "\n",
        "The code creates a folder named `my_model`, moves the model weights into it, and renames them from `best.pt` to `my_model.pt`, and also adds the training results for reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_notebook_path():\n",
        "    return str(pathlib.Path().resolve())\n",
        "\n",
        "print(get_notebook_path())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcBdnOA9v85S"
      },
      "outputs": [],
      "source": [
        "base_dir = get_notebook_path()\n",
        "train_dir = os.path.join(base_dir, \"runs\", \"detect\", \"train\")\n",
        "output_dir = os.path.join(base_dir, \"my_model\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "shutil.copy(os.path.join(train_dir, \"weights\", \"best.pt\"), os.path.join(output_dir, \"my_model.pt\"))\n",
        "shutil.copytree(train_dir, os.path.join(output_dir, \"train\"), dirs_exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Using the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ WARN:0@1.615] global cap_v4l.cpp:914 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
            "[ERROR:0@1.616] global obsensor_uvc_stream_channel.cpp:163 getStreamChannelGroup Camera index out of range\n",
            "Unable to read frames from the camera. This indicates the camera is disconnected or not working. Exiting program.\n",
            "Average pipeline FPS: 0.00\n"
          ]
        }
      ],
      "source": [
        "!python utils/yolo_detect.py --model my_model/my_model.pt --source usb0 --resolution 1280x720"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
